\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{listings}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Real-Time Face and Gesture Recognition with Intelligent Association}

\author{\IEEEauthorblockN{Your Name}
\IEEEauthorblockA{\textit{Department Name} \\
\textit{University Name}\\
City, Country \\
email@example.com}
}

\maketitle

\begin{abstract}
This paper presents a real-time computer vision system that integrates face recognition with hand gesture detection to identify both individuals and their corresponding gestures. The system combines classical computer vision techniques for face recognition using Local Binary Pattern Histogram (LBPH) with modern deep learning approaches for gesture classification using ONNX neural networks. A novel spatial association algorithm links detected gestures to specific individuals based on proximity analysis. The system achieves real-time performance of 15-30 frames per second while simultaneously tracking multiple faces and hands. Experimental results demonstrate robust recognition accuracy and effective person-gesture association in multi-person scenarios. The system filters 44 possible gestures to three essential actions: peace, thumbs up, and stop, making it suitable for practical applications in human-computer interaction, accessibility, and interactive presentations.
\end{abstract}

\begin{IEEEkeywords}
face recognition, gesture recognition, computer vision, real-time tracking, human-computer interaction, ONNX, LBPH
\end{IEEEkeywords}

\section{Introduction}
Human-computer interaction has evolved significantly with advances in computer vision and machine learning. The ability to recognize both individuals and their gestures simultaneously opens new possibilities for personalized interactive systems. Traditional systems typically address face recognition or gesture recognition independently, lacking the capability to associate specific gestures with identified individuals.

This work presents an integrated system that combines face recognition with hand gesture detection and introduces a spatial association mechanism to link gestures to specific people. The system operates in real-time, processing video streams at 15-30 frames per second while maintaining high accuracy in both recognition tasks.

The primary contributions of this work include: a hybrid architecture combining classical and modern computer vision techniques, a spatial association algorithm for linking gestures to individuals, efficient real-time processing with multi-person support, and a practical implementation using open-source tools and pre-trained models.

\section{Related Work}
Face recognition has been extensively studied with various approaches ranging from classical methods like Eigenfaces and Fisherfaces to modern deep learning techniques. LBPH remains popular for real-time applications due to its computational efficiency and robustness to illumination changes.


Gesture recognition has evolved from template matching and hidden Markov models to convolutional neural networks and transformer architectures. Recent datasets like HaGRID provide large-scale annotated hand gesture data enabling robust deep learning models.

Multi-modal systems combining face and gesture recognition have been explored primarily in security and authentication contexts. However, most existing systems process these modalities independently without establishing associations between recognized individuals and their gestures. Our work addresses this gap by introducing spatial proximity-based association.

\section{System Architecture}

\subsection{Overview}
The system architecture consists of three main components: face detection and recognition module, hand detection and gesture classification module, and spatial association module. The pipeline processes each video frame through parallel face and gesture recognition paths before applying the association algorithm.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{system_architecture.png}}
\caption{System architecture showing parallel processing of face and gesture recognition with spatial association.}
\label{fig:architecture}
\end{figure}

\subsection{Face Recognition Module}
The face recognition module employs a two-stage approach. First, Haar Cascade classifiers detect face regions in the input frame. These classifiers use integral images and AdaBoost learning to achieve real-time detection performance. The cascade structure allows rapid rejection of non-face regions while maintaining high detection rates.

Once faces are detected, the LBPH algorithm performs recognition. LBPH divides the face region into cells and computes local binary patterns for each cell, creating a histogram representation. This approach provides robustness to monotonic illumination changes and computational efficiency suitable for real-time applications.

The recognizer is trained on a dataset of nine individuals with 120 images per person captured under varying conditions. The training process creates a model that can identify known individuals with confidence scores, rejecting unknown faces when confidence falls below a threshold of 100.

\subsection{Gesture Recognition Module}
The gesture recognition module utilizes two ONNX neural networks: a hand detector and a gesture classifier. The detector localizes hand regions in the frame using a single-shot detection architecture, providing bounding boxes and confidence scores for detected hands.

The classifier processes cropped hand regions to identify specific gestures. While the original model supports 44 gesture classes from the HaGRID dataset, our implementation filters these to three essential gestures: peace, thumbs up, and stop. This filtering reduces false positives and focuses on gestures with clear semantic meaning for interaction scenarios.

\subsection{Hand Tracking}
To maintain temporal consistency and enable gesture sequence recognition, the system implements OC-SORT tracking algorithm. This algorithm combines Kalman filtering for motion prediction with intersection-over-union matching for data association.

Each tracked hand maintains a deque of recent observations with configurable maximum length. The tracker handles occlusions and temporary detection failures by predicting hand positions during missing observations. Track management includes creation of new tracks for unmatched detections and removal of tracks that exceed maximum age without updates.

\subsection{Spatial Association Algorithm}
The spatial association module links detected gestures to nearby faces using Euclidean distance in image coordinates. For each gesture action, the algorithm computes distances to all detected face centers. If the minimum distance falls below a threshold of 200 pixels, the gesture is associated with that individual.

This approach assumes that people typically make gestures near their body, particularly their face. The threshold value balances association accuracy with robustness to varying camera distances and frame compositions. Gestures without nearby faces are reported as generic detections without person attribution.

\section{Implementation Details}

\subsection{Software Framework}
The system is implemented in Python using OpenCV for image processing and face recognition, ONNX Runtime for neural network inference, NumPy for numerical computations, and SciPy for scientific computing utilities.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{detection_example.png}}
\caption{Example detection showing face recognition with bounding boxes and gesture detection with associated person labels.}
\label{fig:detection}
\end{figure}

\subsection{Model Configuration}
The hand detector operates at input resolution of 1280x720 pixels, providing a balance between detection accuracy and processing speed. The gesture classifier processes cropped hand regions normalized to the model's expected input dimensions.

Face detection uses a scale factor of 1.3 and minimum neighbors of 5 to balance detection sensitivity and false positive rate. Minimum face size is set to 30x30 pixels to avoid detecting distant or partial faces.

\subsection{Tracking Parameters}
The OC-SORT tracker uses maximum age of 30 frames, allowing tracks to persist through brief occlusions. Minimum hits of 3 frames confirms tracks before reporting, reducing false positives from spurious detections. The IOU threshold of 0.3 determines matching sensitivity between detections and predicted track positions.

\subsection{Performance Optimization}
Several optimizations enable real-time performance. Frame mirroring provides natural interaction where users see themselves as in a mirror. Efficient numpy operations minimize computational overhead. The system processes frames sequentially without buffering, maintaining low latency suitable for interactive applications.

\section{Experimental Results}

\subsection{Dataset and Training}
Face recognition training used 1080 images across nine individuals, with 120 images per person. Images were captured under varying lighting conditions, facial expressions, and head poses to ensure robust recognition.

Gesture recognition leverages pre-trained models from the HaGRID dataset, which contains over 550,000 images across 44 gesture classes. Our filtered subset focuses on three gestures with high recognition accuracy and clear semantic meaning.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{multi_person.png}}
\caption{Multi-person scenario demonstrating simultaneous face and gesture recognition with correct associations.}
\label{fig:multiperson}
\end{figure}

\subsection{Recognition Accuracy}
Face recognition achieves high accuracy for trained individuals under normal lighting conditions. The LBPH algorithm demonstrates robustness to minor pose variations and expression changes. Unknown faces are correctly rejected when confidence scores exceed the threshold.

Gesture recognition accuracy benefits from the pre-trained models' extensive training on HaGRID. The three selected gestures show distinct visual features enabling reliable classification. Filtering to these gestures significantly reduces false positives compared to using all 44 classes.

\subsection{Association Performance}
The spatial association algorithm successfully links gestures to individuals in typical interaction scenarios. The 200-pixel threshold accommodates various camera distances while maintaining association accuracy. In multi-person scenarios, the algorithm correctly attributes gestures to the nearest individual.

Edge cases include gestures made far from the body and multiple people in close proximity. The system handles these by either reporting unassociated gestures or associating with the nearest face, which typically produces intuitive results.

\subsection{Real-Time Performance}
The system achieves 15-30 frames per second on modern hardware with CPU inference. Performance varies with the number of detected faces and hands, as processing time scales with detection count. Single-person scenarios typically achieve higher frame rates than multi-person scenes.


Memory usage remains modest due to efficient data structures and the absence of frame buffering. The system can run continuously without memory leaks or performance degradation over extended periods.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{gesture_examples.png}}
\caption{Examples of the three recognized gestures: peace, thumbs up, and stop, with corresponding detection outputs.}
\label{fig:gestures}
\end{figure}

\subsection{Robustness Analysis}
The system demonstrates robustness to various challenging conditions. Partial occlusions of hands are handled through temporal tracking, maintaining gesture recognition when hands are briefly obscured. Varying lighting conditions affect face recognition more than gesture recognition, as the neural networks show greater illumination invariance than LBPH.

Background complexity has minimal impact on detection accuracy due to the discriminative power of the trained models. Fast hand movements occasionally cause motion blur, reducing gesture classification confidence but rarely causing misclassification among the three selected gestures.

\section{Applications}

\subsection{Interactive Presentations}
The system enables presenters to receive real-time feedback from specific audience members. Instructors can identify which students are signaling questions or understanding through gestures, facilitating more responsive teaching.

\subsection{Accessibility}
Person-specific gesture recognition supports customized accessibility interfaces. The system can recognize different gesture vocabularies for different users, accommodating individual preferences and abilities.

\subsection{Security and Authentication}
Combining face recognition with gesture-based authentication provides multi-factor security. Users must both be recognized and perform correct gesture sequences, increasing security beyond face recognition alone.

\subsection{Gaming and Entertainment}
Person-aware gesture controls enable multi-player games where each player's gestures control their character. The system distinguishes between players' actions even when multiple people are visible simultaneously.

\section{Discussion}

\subsection{Advantages}
The hybrid approach combining classical and modern techniques leverages the strengths of each. LBPH provides efficient face recognition suitable for real-time applications, while neural networks deliver robust gesture classification. The spatial association algorithm adds person-awareness without requiring additional sensors or complex multi-modal fusion.

The modular architecture facilitates customization and extension. New gestures can be added by modifying the filtering logic, and the face recognition database can be updated through retraining. The system's reliance on standard webcams makes it accessible and deployable without specialized hardware.

\subsection{Limitations}
The spatial association algorithm assumes people make gestures near their faces, which may not hold for all interaction scenarios. Users making gestures while pointing at distant objects or with arms fully extended may not be correctly associated.

Face recognition accuracy depends on training data quality and coverage. Individuals not in the training set are classified as unknown, requiring system updates to recognize new users. The LBPH algorithm's sensitivity to significant pose variations limits recognition when faces are viewed from extreme angles.

Processing multiple people simultaneously increases computational load, potentially reducing frame rate. The system's performance scales with detection count, making it less suitable for crowded scenes with many visible faces and hands.

\subsection{Future Work}
Several enhancements could improve system capabilities. Incorporating depth information from RGB-D cameras would enable more accurate spatial association and gesture recognition. The depth dimension would help disambiguate overlapping hands and provide scale-invariant features.

Replacing LBPH with modern face recognition networks could improve accuracy and robustness to pose variations. However, this would increase computational requirements, potentially necessitating GPU acceleration for real-time performance.

Expanding the gesture vocabulary while maintaining low false positive rates requires careful selection and potentially user-specific calibration. Adaptive thresholds based on user behavior could improve recognition accuracy over time.

Temporal gesture recognition beyond single-frame classification would enable recognition of dynamic gestures and gesture sequences. This would support more complex interaction vocabularies and reduce false positives from transitional hand poses.

\section{Conclusion}
This work presents a real-time system integrating face recognition with hand gesture detection and introducing spatial association to link gestures to specific individuals. The hybrid architecture combines classical computer vision for face recognition with modern neural networks for gesture classification, achieving real-time performance while maintaining high accuracy.

The spatial association algorithm successfully attributes gestures to nearby individuals using simple distance-based heuristics, enabling person-aware interaction without complex multi-modal fusion. Experimental results demonstrate robust recognition and effective association in multi-person scenarios.

The system's modular design and reliance on standard hardware make it accessible for various applications including interactive presentations, accessibility interfaces, security systems, and entertainment. Future enhancements incorporating depth sensing and advanced face recognition could further improve capabilities while maintaining real-time performance.

The integration of person identification with gesture recognition represents a step toward more natural and personalized human-computer interaction, where systems can understand not just what actions are performed but who is performing them.

\begin{thebibliography}{00}
\bibitem{b1} T. Ahonen, A. Hadid, and M. Pietikainen, ``Face description with local binary patterns: Application to face recognition,'' IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 28, no. 12, pp. 2037-2041, 2006.

\bibitem{b2} P. Viola and M. Jones, ``Rapid object detection using a boosted cascade of simple features,'' Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, vol. 1, pp. I-I, 2001.

\bibitem{b3} A. Kapitanov et al., ``HaGRID - HAnd Gesture Recognition Image Dataset,'' arXiv preprint arXiv:2206.08219, 2022.

\bibitem{b4} Y. Cao et al., ``Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking,'' arXiv preprint arXiv:2203.14360, 2022.

\bibitem{b5} R. E. Kalman, ``A new approach to linear filtering and prediction problems,'' Journal of Basic Engineering, vol. 82, no. 1, pp. 35-45, 1960.

\bibitem{b6} J. Redmon et al., ``You only look once: Unified, real-time object detection,'' Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788, 2016.

\bibitem{b7} K. He et al., ``Deep residual learning for image recognition,'' Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016.

\bibitem{b8} S. Ren et al., ``Faster R-CNN: Towards real-time object detection with region proposal networks,'' Advances in Neural Information Processing Systems, vol. 28, 2015.

\bibitem{b9} C. Szegedy et al., ``Going deeper with convolutions,'' Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9, 2015.

\bibitem{b10} A. Krizhevsky, I. Sutskever, and G. E. Hinton, ``ImageNet classification with deep convolutional neural networks,'' Advances in Neural Information Processing Systems, vol. 25, 2012.
\end{thebibliography}

\end{document}
